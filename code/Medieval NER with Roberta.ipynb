{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["The following code performs NER on medieval spanish documents by using the model trained on Roberta multilingual by *magistermilitum*: https://huggingface.co/magistermilitum/roberta-multilingual-medieval-ner ."],"metadata":{"id":"IioNxRJQ0Oub"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-kOP2D4yDMTJ","executionInfo":{"status":"ok","timestamp":1745317921520,"user_tz":-120,"elapsed":17240,"user":{"displayName":"Enrique José Rodríguez Martín","userId":"12671130087363149637"}},"outputId":"f0746b48-a129-4683-8558-7270dc075e2c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2b5iUqlNkBZ0"},"outputs":[],"source":["import torch\n","from transformers import pipeline\n","\n","pipe = pipeline(\"token-classification\", model=\"magistermilitum/roberta-multilingual-medieval-ner\")\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Change the working directory to the project folder\n","%cd \"/content/drive/MyDrive/directory/output\""],"metadata":{"id":"OI8-hhPsqJ-j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk, os, json, re\n","nltk.download('punkt')"],"metadata":{"id":"gXZTWGcqq82v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The following piece of code is as suggested by the author of the model in order to obtain the BIO-tagged tokens."],"metadata":{"id":"zJOqQrhx16Wn"}},{"cell_type":"code","source":["\n","\n","class TextProcessor:\n","    def __init__(self, filename):\n","        self.filename = filename\n","        self.sent_detector = nltk.data.load(\"tokenizers/punkt/english.pickle\") #sentence tokenizer\n","        self.sentences = []\n","        self.new_sentences = []\n","        self.results = []\n","        self.new_sentences_token_info = []\n","        self.new_sentences_bio = []\n","        self.BIO_TAGS = []\n","        self.stripped_BIO_TAGS = []\n","\n","    def read_file(self):\n","        #Reading a txt file with one document per line.\n","        with open(self.filename, 'r') as f:\n","            text = f.read()\n","            text = re.sub(r'(?<=[.,])(?=[^\\s])', r' ', text)\n","        self.sentences = self.sent_detector.tokenize(text.strip())\n","\n","    def process_sentences(self): #We split long sentences as encoder has a 256 max-lenght. Sentences with les of 40 words will be merged.\n","        for sentence in self.sentences:\n","            if len(sentence.split()) < 40 and self.new_sentences:\n","                self.new_sentences[-1] += \" \" + sentence\n","            else:\n","                self.new_sentences.append(sentence)\n","\n","    def apply_model(self, pipe):\n","        self.results = list(map(pipe, self.new_sentences))\n","        self.results=[[[y[\"entity\"],y[\"word\"], y[\"start\"], y[\"end\"]] for y in x] for x in self.results]\n","\n","    def tokenize_sentences(self):\n","        sent_num = 0\n","        for n_s in self.new_sentences:\n","            tokens=n_s.split() # Basic tokenization\n","            token_info = []\n","\n","            # Initialize a variable to keep track of character index\n","            char_index = 0\n","            # Iterate through the tokens and record start and end info\n","            for token in tokens:\n","                start = char_index\n","                end = char_index + len(token)  # Subtract 1 for the last character of the token\n","                token_info.append((token, start, end, sent_num))\n","\n","                char_index += len(token) + 1  # Add 1 for the whitespace\n","            self.new_sentences_token_info.append(token_info)\n","            sent_num = sent_num + 1\n","\n","    def process_results(self): #merge subwords and BIO tags\n","        for result in self.results:\n","            merged_bio_result = []\n","            current_word = \"\"\n","            current_label = None\n","            current_start = None\n","            current_end = None\n","            for entity, subword, start, end in result:\n","                if subword.startswith(\"▁\"):\n","                    subword = subword[1:]\n","                    merged_bio_result.append([current_word, current_label, current_start, current_end])\n","                    current_word = \"\" ; current_label = None ; current_start = None ; current_end = None\n","                if current_start is None:\n","                    current_word = subword ; current_label = entity ; current_start = start+1 ; current_end= end\n","                else:\n","                    current_word += subword ; current_end = end\n","            if current_word:\n","                merged_bio_result.append([current_word, current_label, current_start, current_end])\n","            self.new_sentences_bio.append(merged_bio_result[1:])\n","\n","    def match_tokens_with_entities(self): #match BIO tags with tokens\n","        for i,ss in enumerate(self.new_sentences_token_info):\n","            for word in ss:\n","                for ent in self.new_sentences_bio[i]:\n","                    if word[1]==ent[2] or word[1] + 1 == ent[2]:\n","                        if ent[1]==\"L-PERS\":\n","                            self.BIO_TAGS.append([word[0], \"I-PERS\", \"B-LOC\", ent[2], ent[3], word[3]])\n","                            break\n","                        else:\n","                            if \"LOC\" in ent[1]:\n","                                self.BIO_TAGS.append([word[0], \"O\", ent[1], ent[2], ent[3], word[3]])\n","                            else:\n","                                self.BIO_TAGS.append([word[0], ent[1], \"O\", ent[2], ent[3], word[3]])\n","                            break\n","                else:\n","                    self.BIO_TAGS.append([word[0], \"O\", \"O\", 0, 0, word[3]])\n","\n","    def separate_dots_and_comma(self): #optional\n","        signs=[\",\", \";\", \":\", \".\"]\n","        for bio in self.BIO_TAGS:\n","            if any(bio[0][-1]==sign for sign in signs) and len(bio[0])>1:\n","                self.stripped_BIO_TAGS.append([bio[0][:-1], bio[1], bio[2], bio[3], bio[4], bio[5]]);\n","                self.stripped_BIO_TAGS.append([bio[0][-1], \"O\", \"O\"])\n","            else:\n","                self.stripped_BIO_TAGS.append(bio)\n","\n","    def save_BIO(self, id):\n","        with open('/content/drive/MyDrive/directory/output/bio/output_BIO_' + id + '.txt', 'w', encoding='utf-8') as output_file:\n","            output = {}\n","            output[\"entities\"] = []\n","            for x in self.stripped_BIO_TAGS:\n","              if x[1] != \"O\" or x[2] != \"O\":\n","                output[\"entities\"].append({\n","                    \"token\": x[0],\n","                    \"pers\": x[1],\n","                    \"locs\": x[2],\n","                    \"start\": x[3],\n","                    \"end\": x[4],\n","                    \"sent\": x[5]\n","                })\n","            output[\"tokens\"] = self.new_sentences_token_info\n","            json.dump(output, output_file, indent=4, ensure_ascii=False)\n","\n"],"metadata":{"id":"81uYR5V1mwFn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Usage:\n","for filename in os.listdir(\"/content/drive/MyDrive/directory/dataset\"):\n","    print(\"\\nProcesando \" + filename + \"...\")\n","    processor = TextProcessor(\"/content/drive/MyDrive/directory/dataset/\" + filename)\n","    processor.read_file()\n","    processor.process_sentences()\n","    processor.apply_model(pipe)\n","    processor.tokenize_sentences()\n","    processor.process_results()\n","    processor.match_tokens_with_entities()\n","    processor.separate_dots_and_comma()\n","    processor.save_BIO(filename[:-4])"],"metadata":{"id":"e2Ce_P-bWsWU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Some postprocessing is performed in order to obtain fully fledged entities from the BIO-tagged tokens obtained in the previous step. Some entities combine different types of tag -i.e. persons which include locations as part of their name (Alfonso de San Romano)-."],"metadata":{"id":"BTCx7GBP1JH8"}},{"cell_type":"code","source":["entities = {}\n","for file in os.listdir(\"/content/drive/MyDrive/directory/output/bio\"):\n","  with open(\"/content/drive/MyDrive/directory/output/bio/\" + file, 'r', encoding=\"utf8\") as f:\n","        print(\"\\nProcesando \" + file + \"...\")\n","        doc_entities = []\n","        file_data = json.load(f)\n","        data = file_data[\"entities\"]\n","        last_index_per = -1\n","        last_index_loc = -1\n","        b_loc_as_ip = False\n","        for i in range(len(data)):\n","          ent = data[i]\n","          per = ent['pers']\n","          loc = ent['locs']\n","          if 'I' in per and 'B' in loc:\n","            b_ent = data[last_index_per] #Pointer to anchor entity\n","            b_ent['token'] += \" \" + ent['token']\n","            b_ent['end'] = ent['end']\n","            if not b_loc_as_ip:\n","              last_index_loc = i\n","              b_loc_as_ip = True\n","            else:\n","              # Complex B-LOC as I-PERS (de San Romano)\n","              b_ent = data[last_index_loc] #Pointer to anchor entity\n","              b_ent['token'] += \" \" + ent['token']\n","              b_ent['end'] = ent['end']\n","              ent['locs'] = \"\" #To be ignored in next loop\n","          elif 'B' in per:\n","            b_loc_as_ip = False\n","            last_index_per = i\n","          elif 'B' in loc:\n","            b_loc_as_ip = False\n","            last_index_loc = i\n","          elif 'I' in per:\n","            b_ent = data[last_index_per] #Pointer to anchor entity\n","            b_ent['token'] += \" \" + ent['token']\n","            b_ent['end'] = ent['end']\n","          else:\n","            b_ent = data[last_index_loc] #Pointer to anchor entity\n","            b_ent['token'] += \" \" + ent['token']\n","            b_ent['end'] = ent['end']\n","\n","        for i in range(len(data)):\n","          ent = data[i]\n","          per = ent['pers']\n","          loc = ent['locs']\n","          if 'B' in per or 'B' in loc:\n","            label = 'PERS' if 'B-PERS' in per else 'LOC'\n","            doc_entities.append({\n","                \"name\": ent['token'],\n","                \"label\": label,\n","                \"start\": ent['start'],\n","                \"end\": ent['end'],\n","                \"sent\": ent[\"sent\"]\n","            })\n","\n","        entities[file] = {}\n","        entities[file][\"entities\"] = doc_entities\n","        entities[file][\"tokens\"] = sum(file_data[\"tokens\"], [])\n","\n","\n","with open('/content/drive/MyDrive/directory/output/ner.json', 'w', encoding='utf-8') as output_file:\n","  json.dump(entities, output_file, indent=4, ensure_ascii=False)\n"],"metadata":{"id":"V_mFIIQZR9wY"},"execution_count":null,"outputs":[]}]}