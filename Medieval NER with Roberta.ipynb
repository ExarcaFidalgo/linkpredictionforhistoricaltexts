{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOPqVSEnDGJZpdIveA2jZeX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["As an example of the necessary input for the link prediction on historical texts, the following code performs NER on medieval spanish documents by using the model trained on Roberta multilingual by *magistermilitum*: https://huggingface.co/magistermilitum/roberta-multilingual-medieval-ner ."],"metadata":{"id":"IioNxRJQ0Oub"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2b5iUqlNkBZ0","executionInfo":{"status":"ok","timestamp":1719584160419,"user_tz":-120,"elapsed":8456,"user":{"displayName":"Patriarca Oliverio","userId":"08357700772011486937"}},"outputId":"974e0957-8c5f-41bc-aaaf-f52d645220ad"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at magistermilitum/roberta-multilingual-medieval-ner were not used when initializing XLMRobertaForTokenClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["import torch\n","from transformers import pipeline\n","\n","pipe = pipeline(\"token-classification\", model=\"magistermilitum/roberta-multilingual-medieval-ner\")\n"]},{"cell_type":"markdown","source":["Importing Google Drive for loading the necessary files. The example files are available at https://github.com/ExarcaFidalgo/linkpredictionforhistoricaltexts/tree/master/data"],"metadata":{"id":"_BxnNjx20mMY"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Change the working directory to the project folder\n","%cd \"/content/drive/MyDrive/LinkPrediction/output\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OI8-hhPsqJ-j","executionInfo":{"status":"ok","timestamp":1719584162338,"user_tz":-120,"elapsed":1921,"user":{"displayName":"Patriarca Oliverio","userId":"08357700772011486937"}},"outputId":"c85cfdd4-9337-4ce3-da3d-a2b06e43a910"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/LinkPrediction/output\n"]}]},{"cell_type":"code","source":["import nltk, os, json, re\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gXZTWGcqq82v","executionInfo":{"status":"ok","timestamp":1719584162338,"user_tz":-120,"elapsed":4,"user":{"displayName":"Patriarca Oliverio","userId":"08357700772011486937"}},"outputId":"465db1e8-1a5e-4bb4-f5ba-64de827700f7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["The following piece of code is as suggested by the author of the model in order to obtain the BIO-tagged tokens."],"metadata":{"id":"zJOqQrhx16Wn"}},{"cell_type":"code","source":["\n","\n","class TextProcessor:\n","    def __init__(self, filename):\n","        self.filename = filename\n","        self.sent_detector = nltk.data.load(\"tokenizers/punkt/english.pickle\") #sentence tokenizer\n","        self.sentences = []\n","        self.new_sentences = []\n","        self.results = []\n","        self.new_sentences_token_info = []\n","        self.new_sentences_bio = []\n","        self.BIO_TAGS = []\n","        self.stripped_BIO_TAGS = []\n","\n","    def read_file(self):\n","        #Reading a txt file with one document per line.\n","        with open(self.filename, 'r') as f:\n","            text = f.read()\n","            text = re.sub(r'(?<=[.,])(?=[^\\s])', r' ', text)\n","        self.sentences = self.sent_detector.tokenize(text.strip())\n","\n","    def process_sentences(self): #We split long sentences as encoder has a 256 max-lenght. Sentences with les of 40 words will be merged.\n","        for sentence in self.sentences:\n","            if len(sentence.split()) < 40 and self.new_sentences:\n","                self.new_sentences[-1] += \" \" + sentence\n","            else:\n","                self.new_sentences.append(sentence)\n","\n","    def apply_model(self, pipe):\n","        self.results = list(map(pipe, self.new_sentences))\n","        self.results=[[[y[\"entity\"],y[\"word\"], y[\"start\"], y[\"end\"]] for y in x] for x in self.results]\n","\n","    def tokenize_sentences(self):\n","        sent_num = 0\n","        for n_s in self.new_sentences:\n","            tokens=n_s.split() # Basic tokenization\n","            token_info = []\n","\n","            # Initialize a variable to keep track of character index\n","            char_index = 0\n","            # Iterate through the tokens and record start and end info\n","            for token in tokens:\n","                start = char_index\n","                end = char_index + len(token)  # Subtract 1 for the last character of the token\n","                token_info.append((token, start, end, sent_num))\n","\n","                char_index += len(token) + 1  # Add 1 for the whitespace\n","            self.new_sentences_token_info.append(token_info)\n","            sent_num = sent_num + 1\n","\n","    def process_results(self): #merge subwords and BIO tags\n","        for result in self.results:\n","            merged_bio_result = []\n","            current_word = \"\"\n","            current_label = None\n","            current_start = None\n","            current_end = None\n","            for entity, subword, start, end in result:\n","                if subword.startswith(\"▁\"):\n","                    subword = subword[1:]\n","                    merged_bio_result.append([current_word, current_label, current_start, current_end])\n","                    current_word = \"\" ; current_label = None ; current_start = None ; current_end = None\n","                if current_start is None:\n","                    current_word = subword ; current_label = entity ; current_start = start+1 ; current_end= end\n","                else:\n","                    current_word += subword ; current_end = end\n","            if current_word:\n","                merged_bio_result.append([current_word, current_label, current_start, current_end])\n","            self.new_sentences_bio.append(merged_bio_result[1:])\n","\n","    def match_tokens_with_entities(self): #match BIO tags with tokens\n","        for i,ss in enumerate(self.new_sentences_token_info):\n","            for word in ss:\n","                for ent in self.new_sentences_bio[i]:\n","                    if word[1]==ent[2] or word[1] + 1 == ent[2]:\n","                        if ent[1]==\"L-PERS\":\n","                            self.BIO_TAGS.append([word[0], \"I-PERS\", \"B-LOC\", ent[2], ent[3], word[3]])\n","                            break\n","                        else:\n","                            if \"LOC\" in ent[1]:\n","                                self.BIO_TAGS.append([word[0], \"O\", ent[1], ent[2], ent[3], word[3]])\n","                            else:\n","                                self.BIO_TAGS.append([word[0], ent[1], \"O\", ent[2], ent[3], word[3]])\n","                            break\n","                else:\n","                    self.BIO_TAGS.append([word[0], \"O\", \"O\", 0, 0, word[3]])\n","\n","    def separate_dots_and_comma(self): #optional\n","        signs=[\",\", \";\", \":\", \".\"]\n","        for bio in self.BIO_TAGS:\n","            if any(bio[0][-1]==sign for sign in signs) and len(bio[0])>1:\n","                self.stripped_BIO_TAGS.append([bio[0][:-1], bio[1], bio[2], bio[3], bio[4], bio[5]]);\n","                self.stripped_BIO_TAGS.append([bio[0][-1], \"O\", \"O\"])\n","            else:\n","                self.stripped_BIO_TAGS.append(bio)\n","\n","    def save_BIO(self, id):\n","        with open('/content/drive/MyDrive/LinkPrediction/output/bio/output_BIO_' + id + '.txt', 'w', encoding='utf-8') as output_file:\n","            output = {}\n","            output[\"entities\"] = []\n","            for x in self.stripped_BIO_TAGS:\n","              if x[1] != \"O\" or x[2] != \"O\":\n","                output[\"entities\"].append({\n","                    \"token\": x[0],\n","                    \"pers\": x[1],\n","                    \"locs\": x[2],\n","                    \"start\": x[3],\n","                    \"end\": x[4],\n","                    \"sent\": x[5]\n","                })\n","            output[\"tokens\"] = self.new_sentences_token_info\n","            json.dump(output, output_file, indent=4, ensure_ascii=False)\n","\n"],"metadata":{"id":"81uYR5V1mwFn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The dataset has been obtained from the documents listed in the doctoral thesis of Jorge Felpeto Cueva: https://digibuo.uniovi.es/dspace/bitstream/handle/10651/71402/TD_JorgeFelpetoCueva.pdf?sequence=1&isAllowed=y ."],"metadata":{"id":"Q1tfK2Yv07Bf"}},{"cell_type":"code","source":["# Usage:\n","for filename in os.listdir(\"/content/drive/MyDrive/LinkPrediction/test_dataset\"):\n","    print(\"\\nProcesando \" + filename + \"...\")\n","    processor = TextProcessor(\"/content/drive/MyDrive/LinkPrediction/test_dataset/\" + filename)\n","    processor.read_file()\n","    processor.process_sentences()\n","    processor.apply_model(pipe)\n","    processor.tokenize_sentences()\n","    processor.process_results()\n","    processor.match_tokens_with_entities()\n","    processor.separate_dots_and_comma()\n","    processor.save_BIO(filename[:-4])"],"metadata":{"id":"e2Ce_P-bWsWU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719585536299,"user_tz":-120,"elapsed":1373963,"user":{"displayName":"Patriarca Oliverio","userId":"08357700772011486937"}},"outputId":"8fcc60d9-a606-4b46-dff6-771fafbea54d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Procesando AMSPO_FSV_1552.txt...\n","\n","Procesando AMSPO_FSV_1377.txt...\n","\n","Procesando AMSPO_FSV_1553.txt...\n","\n","Procesando AMSPO_FSV_1355.txt...\n","\n","Procesando AMSPO_FSV_1540.txt...\n","\n","Procesando AMSPO_FSV_1554.txt...\n","\n","Procesando AMSPO_FSV_1577.txt...\n","\n","Procesando AMSPO_FSV_1555.txt...\n","\n","Procesando AMSPO_FSP_306.txt...\n","\n","Procesando AMSPO_FSV_1551.txt...\n","\n","Procesando AMSPO_FSV_1564.txt...\n","\n","Procesando AMSPO_FSV_1567.txt...\n","\n","Procesando AMSPO_FSV_1572.txt...\n","\n","Procesando AMSPO_FSV_1580.txt...\n","\n","Procesando AMSPO_FSV_1576.txt...\n","\n","Procesando AMSPO_FSV_1561.txt...\n","\n","Procesando AMSPO_FSV_1558.txt...\n","\n","Procesando AMSPO_FSV_1560.txt...\n","\n","Procesando AMSPO_FSV_1569.txt...\n","\n","Procesando AMSPO_FSV_1574.txt...\n","\n","Procesando AMSPO_FSV_1570.txt...\n","\n","Procesando AMSPO_FSV_1559.txt...\n","\n","Procesando AMSPO_FSV_1602.txt...\n","\n","Procesando AMSPO_FSV_1591b.txt...\n","\n","Procesando AMSPO_FSV_1589.txt...\n","\n","Procesando AMSPO_FSV_1603.txt...\n","\n","Procesando AMSPO_FSV_1601.txt...\n","\n","Procesando AMSPO_FSV_1599.txt...\n","\n","Procesando AMSPO_FSV_1588.txt...\n","\n","Procesando AMSPO_FSV_1598.txt...\n","\n","Procesando AMSPO_FSV_1582.txt...\n","\n","Procesando AMSPO_FSV_1596.txt...\n","\n","Procesando AMSPO_FSV_1593.txt...\n","\n","Procesando AMSPO_FSV_1594.txt...\n","\n","Procesando AMSPO_FSV_1590.txt...\n","\n","Procesando AMSPO_FSV_1595.txt...\n","\n","Procesando AMSPO_FSV_1597.txt...\n","\n","Procesando AMSPO_FSV_1584.txt...\n","\n","Procesando AMSPO_FSV_1586.txt...\n","\n","Procesando AMSPO_FSV_1620.txt...\n","\n","Procesando AMSPO_FSV_1592.txt...\n","\n","Procesando AMSPO_FSV_1618.txt...\n","\n","Procesando AMSPO_FSV_1624.txt...\n","\n","Procesando AMSPO_FSV_1628.txt...\n","\n","Procesando AMSPO_FSV_1617.txt...\n","\n","Procesando AMSPO_FSV_1614.txt...\n","\n","Procesando AMSPO_FSV_1632.txt...\n","\n","Procesando AMSPO_FSV_1609.txt...\n","\n","Procesando AMSPO_FSV_1629.txt...\n","\n","Procesando AMSPO_FSV_1622.txt...\n","\n","Procesando AMSPO_FSV_1613.txt...\n","\n","Procesando AMSPO_FSV_1612.txt...\n","\n","Procesando AMSPO_FSV_1610.txt...\n","\n","Procesando AMSPO_FSV_1627.txt...\n","\n","Procesando AMSPO_FSV_1631.txt...\n","\n","Procesando AMSPO_FSV_1649.txt...\n","\n","Procesando AMSPO_FSV_1673.txt...\n","\n","Procesando AMSPO_FSV_1650.txt...\n","\n","Procesando AMSPO_FSV_1657.txt...\n","\n","Procesando AMSPO_FSV_1638.txt...\n","\n","Procesando AMSPO_FSV_1653.txt...\n","\n","Procesando AMSPO_FSV_1644.txt...\n","\n","Procesando AMSPO_FSV_1664.txt...\n","\n","Procesando AMSPO_FSV_1677.txt...\n","\n","Procesando AMSPO_FSV_1672.txt...\n","\n","Procesando AMSPO_FSV_1654.txt...\n","\n","Procesando AMSPO_FSV_1640.txt...\n","\n","Procesando AMSPO_FSV_1636.txt...\n","\n","Procesando AMSPO_FSV_1635.txt...\n","\n","Procesando AMSPO_FSV_1656.txt...\n","\n","Procesando AMSPO_FSV_1697.txt...\n","\n","Procesando AMSPO_FSV_1680b.txt...\n","\n","Procesando AMSPO_FSV_1683.txt...\n","\n","Procesando AMSPO_FSV_1682.txt...\n","\n","Procesando AMSPO_FSV_1738.txt...\n","\n","Procesando AMSPO_FSV_1687.txt...\n","\n","Procesando AMSPO_FSV_1684.txt...\n","\n","Procesando AMSPO_FSV_1685.txt...\n","\n","Procesando AMSPO_FSV_1730.txt...\n","\n","Procesando AMSPO_FSV_1679.txt...\n","\n","Procesando AMSPO_FSV_1809.txt...\n","\n","Procesando AMSPO_FSV_1681.txt...\n","\n","Procesando AMSPO_FSV_1722.txt...\n","\n","Procesando AMSPO_FSV_1735.txt...\n","\n","Procesando AMSPO_FSV_1731.txt...\n","\n","Procesando AMSPO_FSV_1607.txt...\n","\n","Procesando AMSPO_FSV_1615.txt...\n","\n","Procesando AMSPO_FSV_1688.txt...\n","\n","Procesando AMSPO_FSV_1591.txt...\n","\n","Procesando AMSPO_FSV_1605.txt...\n","\n","Procesando AMSPO_FSV_1581.txt...\n","\n","Procesando AMSPO_FSV_1600.txt...\n","\n","Procesando AMSPO_FSV_1579.txt...\n","\n","Procesando AMSPO_FSV_1587.txt...\n","\n","Procesando AMSPO_FSV_1583.txt...\n","\n","Procesando AMSPO_FSV_1585.txt...\n","\n","Procesando AMSPO_FSV_1578.txt...\n","\n","Procesando AMSPO_FSV_1568.txt...\n","\n","Procesando AMSPO_FSV_1566.txt...\n","\n","Procesando AMSPO_FSV_1565.txt...\n","\n","Procesando AMSPO_FSV_1563.txt...\n","\n","Procesando AMSPO_FSV_1692.txt...\n","\n","Procesando AMSPO_FSV_1367.txt...\n","\n","Procesando AMSPO_FSV_1696.txt...\n","\n","Procesando AMSPO_FSV_1350.txt...\n","\n","Procesando AMSPO_FSV_1691.txt...\n","\n","Procesando AMSPO_FSV_1695.txt...\n","\n","Procesando AMSPO_FSV_1694.txt...\n","\n","Procesando AMSPO_FSV_1690.txt...\n","\n","Procesando AMSPO_FSV_1687b.txt...\n","\n","Procesando AMSPO_FSV_1678.txt...\n","\n","Procesando AMSPO_FSV_1686.txt...\n","\n","Procesando AMSPO_FSV_1675.txt...\n","\n","Procesando AMSPO_FSV_1680.txt...\n","\n","Procesando AMSPO_FSV_1663.txt...\n","\n","Procesando AMSPO_FSV_1661.txt...\n","\n","Procesando AMSPO_FSV_1652.txt...\n","\n","Procesando AMSPO_FSV_1660.txt...\n","\n","Procesando AMSPO_FSV_1648.txt...\n","\n","Procesando AMSPO_FSV_1651.txt...\n","\n","Procesando AMSPO_FSV_1647.txt...\n","\n","Procesando AMSPO_FSV_1646.txt...\n","\n","Procesando AMSPO_FSV_1642.txt...\n","\n","Procesando AMSPO_FSV_1639.txt...\n","\n","Procesando AMSPO_FSV_1625.txt...\n","\n","Procesando AMSPO_FSV_1630.txt...\n","\n","Procesando AMSPO_FSV_1621.txt...\n","\n","Procesando AMSPO_FSV_1619.txt...\n","\n","Procesando AMSPO_FSV_1608.txt...\n"]}]},{"cell_type":"markdown","source":["Some postprocessing is performed in order to obtain fully fledged entities from the BIO-tagged tokens obtained in the previous step. Some entities combine different types of tag -i.e. persons which include locations as part of their name (Alfonso de San Romano)-."],"metadata":{"id":"BTCx7GBP1JH8"}},{"cell_type":"code","source":["entities = {}\n","for file in os.listdir(\"/content/drive/MyDrive/LinkPrediction/output/bio\"):\n","  with open(\"/content/drive/MyDrive/LinkPrediction/output/bio/\" + file, 'r', encoding=\"utf8\") as f:\n","        print(\"\\nProcesando \" + file + \"...\")\n","        doc_entities = []\n","        file_data = json.load(f)\n","        data = file_data[\"entities\"]\n","        last_index_per = -1\n","        last_index_loc = -1\n","        b_loc_as_ip = False\n","        for i in range(len(data)):\n","          ent = data[i]\n","          per = ent['pers']\n","          loc = ent['locs']\n","          if 'I' in per and 'B' in loc:\n","            b_ent = data[last_index_per] #Pointer to anchor entity\n","            b_ent['token'] += \" \" + ent['token']\n","            b_ent['end'] = ent['end']\n","            if not b_loc_as_ip:\n","              last_index_loc = i\n","              b_loc_as_ip = True\n","            else:\n","              # Complex B-LOC as I-PERS (de San Romano)\n","              b_ent = data[last_index_loc] #Pointer to anchor entity\n","              b_ent['token'] += \" \" + ent['token']\n","              b_ent['end'] = ent['end']\n","              ent['locs'] = \"\" #To be ignored in next loop\n","          elif 'B' in per:\n","            b_loc_as_ip = False\n","            last_index_per = i\n","          elif 'B' in loc:\n","            b_loc_as_ip = False\n","            last_index_loc = i\n","          elif 'I' in per:\n","            b_ent = data[last_index_per] #Pointer to anchor entity\n","            b_ent['token'] += \" \" + ent['token']\n","            b_ent['end'] = ent['end']\n","          else:\n","            b_ent = data[last_index_loc] #Pointer to anchor entity\n","            b_ent['token'] += \" \" + ent['token']\n","            b_ent['end'] = ent['end']\n","\n","        for i in range(len(data)):\n","          ent = data[i]\n","          per = ent['pers']\n","          loc = ent['locs']\n","          if 'B' in per or 'B' in loc:\n","            label = 'PERS' if 'B-PERS' in per else 'LOC'\n","            doc_entities.append({\n","                \"name\": ent['token'],\n","                \"label\": label,\n","                \"start\": ent['start'],\n","                \"end\": ent['end'],\n","                \"sent\": ent[\"sent\"]\n","            })\n","\n","        entities[file] = {}\n","        entities[file][\"entities\"] = doc_entities\n","        entities[file][\"tokens\"] = sum(file_data[\"tokens\"], [])\n","\n","\n","with open('/content/drive/MyDrive/LinkPrediction/output/ner.json', 'w', encoding='utf-8') as output_file:\n","  json.dump(entities, output_file, indent=4, ensure_ascii=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V_mFIIQZR9wY","executionInfo":{"status":"ok","timestamp":1719585538122,"user_tz":-120,"elapsed":1827,"user":{"displayName":"Patriarca Oliverio","userId":"08357700772011486937"}},"outputId":"8bdb2217-5985-4f03-9d5a-8a5c86f20cee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Procesando output_BIO_AMSPO_FSV_1552.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1377.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1553.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1355.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1350.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1540.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1367.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1554.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1577.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1555.txt...\n","\n","Procesando output_BIO_AMSPO_FSP_306.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1551.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1564.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1567.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1572.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1565.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1580.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1576.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1578.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1561.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1568.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1558.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1579.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1566.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1560.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1569.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1585.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1574.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1570.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1559.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1563.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1602.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1591b.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1589.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1603.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1601.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1599.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1588.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1587.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1583.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1581.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1598.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1582.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1596.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1593.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1594.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1590.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1595.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1591.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1600.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1597.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1584.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1586.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1620.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1592.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1605.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1625.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1618.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1624.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1628.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1619.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1617.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1614.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1632.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1609.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1629.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1622.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1630.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1608.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1613.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1612.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1621.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1610.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1627.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1631.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1649.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1673.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1650.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1657.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1638.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1653.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1644.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1663.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1664.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1677.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1660.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1672.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1642.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1661.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1675.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1654.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1647.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1640.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1636.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1651.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1635.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1639.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1656.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1646.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1697.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1695.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1686.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1694.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1680b.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1690.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1683.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1682.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1692.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1738.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1687.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1687b.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1684.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1678.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1685.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1680.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1679.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1809.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1691.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1681.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1722.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1735.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1731.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1607.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1615.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1652.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1648.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1688.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1696.txt...\n","\n","Procesando output_BIO_AMSPO_FSV_1730.txt...\n"]}]}]}